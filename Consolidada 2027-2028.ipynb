{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf5b59c-584b-4e20-905c-e64e5c881de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Análisis de la demanda agregada para la compra 2027-2028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eceb6d4-0366-461d-aebc-7298dae7230e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3918b3d6-f7d0-470a-a240-c3a629d45cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Módulos\n",
    "%pip install openpyxl\n",
    "%pip install pandas\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e6f1c6-ac64-4f98-80f2-32e8e97b1328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Read messy Excel as raw grid (string-first)\n",
    "# -----------------------------\n",
    "xlsx_path = \"/Volumes/workspace/default/eseotres/Análisis Plataforma Salud.xlsx\"\n",
    "\n",
    "pdf = pd.read_excel(\n",
    "    xlsx_path,\n",
    "    sheet_name=\"Info licitación\",\n",
    "    header=None,\n",
    "    dtype=str,\n",
    "    na_filter=False\n",
    ")\n",
    "\n",
    "# Add a true row index BEFORE Spark (preserves order 1:1 with Excel)\n",
    "pdf[\"_row_id\"] = range(len(pdf))\n",
    "\n",
    "# 2) Convert to Spark (keep your existing schema build, but include _row_id)\n",
    "data = pdf.astype(str).values.tolist()  # ok, but we must treat \"nan\" as empty later\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(f\"_c{i}\", StringType(), True) for i in range(pdf.shape[1] - 1)] +\n",
    "    [StructField(\"_row_id\", StringType(), True)]  # as string for now; we cast below\n",
    ")\n",
    "\n",
    "df_bronze = spark.createDataFrame(data, schema=schema).withColumn(\"_row_id\", F.col(\"_row_id\").cast(\"long\"))\n",
    "\n",
    "# 3) Drop ONLY rows that are entirely empty (treat \"\", null, \"nan\", \"none\" as empty)\n",
    "data_cols = [c for c in df_bronze.columns if c != \"_row_id\"]\n",
    "\n",
    "is_empty_col = lambda c: (\n",
    "    F.lower(F.trim(F.coalesce(F.col(c), F.lit(\"\")))).isin(\"\", \"nan\", \"none\", \"null\")\n",
    ")\n",
    "\n",
    "all_empty_expr = reduce(lambda a, b: a & b, [is_empty_col(c) for c in data_cols])\n",
    "\n",
    "df_bronze = df_bronze.filter(~all_empty_expr)\n",
    "\n",
    "# Now you can inspect in the real order:\n",
    "df_bronze.orderBy(\"_row_id\").limit(30).show(truncate=False)\n",
    "print(\"Spark bronze cols:\", len(df_bronze.columns), \"| rows:\", df_bronze.count())\n",
    "\n",
    "# 4) Persist Bronze Delta table\n",
    "# (optional but safest) drop the old table first\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.bronze_licitacion_info\")\n",
    "\n",
    "# write the new one\n",
    "(df_bronze.write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(\"workspace.default.bronze_licitacion_info\"))\n",
    "\n",
    "print(\"✅ Created table: workspace.default.bronze_licitacion_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f37667-1cdc-4f18-a44c-16097c5d5740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4569d7e1-460d-4716-ad85-65ced91ced65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def norm(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = (s.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\")\n",
    "          .replace(\"ó\",\"o\").replace(\"ú\",\"u\").replace(\"ñ\",\"n\"))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def to_double(colname):\n",
    "    return F.when(\n",
    "        F.trim(F.coalesce(F.col(colname), F.lit(\"\"))) == \"\", None\n",
    "    ).otherwise(\n",
    "        F.regexp_replace(F.col(colname), \",\", \"\").cast(\"double\")\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load Bronze\n",
    "# -----------------------------\n",
    "df_bronze = spark.table(\"workspace.default.bronze_licitacion_info\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Detect header rows\n",
    "# -----------------------------\n",
    "data_cols = [c for c in df_bronze.columns if c != \"_row_id\"]\n",
    "\n",
    "contains_clave = reduce(\n",
    "    lambda a, b: a | b,\n",
    "    [F.lower(F.col(c)).contains(\"clave\") for c in data_cols]\n",
    ")\n",
    "\n",
    "header_row_id = df_bronze.where(contains_clave).select(F.min(\"_row_id\")).first()[0]\n",
    "group_row_id  = header_row_id - 1\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Extract header rows to pandas\n",
    "# -----------------------------\n",
    "pdf_head = (\n",
    "    df_bronze\n",
    "    .where(F.col(\"_row_id\").isin([group_row_id, header_row_id]))\n",
    "    .orderBy(\"_row_id\")\n",
    "    .drop(\"_row_id\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "r_group = pdf_head.iloc[0].replace([\"\", \"nan\", \"None\", None], pd.NA).ffill().fillna(\"\")\n",
    "r_head  = pdf_head.iloc[1].fillna(\"\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build FINAL column names (allow duplicates)\n",
    "# -----------------------------\n",
    "final_cols = []\n",
    "for g, h in zip(r_group.tolist(), r_head.tolist()):\n",
    "    g0 = norm(g)\n",
    "    h0 = norm(h)\n",
    "\n",
    "    if h0 in (\"clave\", \"descripcion\"):\n",
    "        final_cols.append(h0)\n",
    "    elif h0 in (\"min\", \"max\") and g0:\n",
    "        final_cols.append(f\"{g0}_{h0}\")\n",
    "    else:\n",
    "        final_cols.append(\"col\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Create structured Silver base\n",
    "# -----------------------------\n",
    "df_data = (\n",
    "    df_bronze\n",
    "    .where(F.col(\"_row_id\") > header_row_id)\n",
    "    .drop(\"_row_id\")\n",
    "    .toDF(*final_cols)\n",
    ")\n",
    "# --- define aggregation prefixes ---\n",
    "# Define the aggregation families\n",
    "AGG_FAMILIES = [\"imss_bienestar\", \"ccinshae\", \"salud_spps\"]\n",
    "\n",
    "# Start with your existing dataframe\n",
    "df_aggregated = df_data\n",
    "\n",
    "# For each family, find matching columns and aggregate them\n",
    "for family in AGG_FAMILIES:\n",
    "    # Find columns that match the pattern: family_*_min and family_*_max\n",
    "    min_cols = [col for col in df_data.columns if col.startswith(f\"{family}_\") and col.endswith(\"_min\")]\n",
    "    max_cols = [col for col in df_data.columns if col.startswith(f\"{family}_\") and col.endswith(\"_max\")]\n",
    "    \n",
    "    # Sum these columns to create aggregated columns\n",
    "    if min_cols:\n",
    "        # Create sum expression for min columns\n",
    "        min_sum_expr = sum([F.col(c) for c in min_cols])\n",
    "        df_aggregated = df_aggregated.withColumn(f\"{family}_min\", min_sum_expr)\n",
    "    \n",
    "    if max_cols:\n",
    "        # Create sum expression for max columns\n",
    "        max_sum_expr = sum([F.col(c) for c in max_cols])\n",
    "        df_aggregated = df_aggregated.withColumn(f\"{family}_max\", max_sum_expr)\n",
    "\n",
    "# Now select only the columns you need\n",
    "# Get the base columns\n",
    "base_cols = ['clave', 'descripcion', 'imss_min', 'imss_max', 'issste_min', 'issste_max', \n",
    "             'pemex_min', 'pemex_max']\n",
    "\n",
    "# Add the aggregated family columns\n",
    "for family in AGG_FAMILIES:\n",
    "    base_cols.extend([f\"{family}_min\", f\"{family}_max\"])\n",
    "\n",
    "# Add totales if they exist\n",
    "if 'totales_min' in df_data.columns:\n",
    "    base_cols.extend(['totales_min', 'totales_max'])\n",
    "\n",
    "# Create the final dataframe by selecting only the needed columns\n",
    "df_final = df_aggregated.select(base_cols)\n",
    "\n",
    "df_final.display()\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Persist Silver\n",
    "# -----------------------------\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.silver_licitacion_info\")\n",
    "\n",
    "(df_final.write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(\"workspace.default.silver_licitacion_info\"))\n",
    "\n",
    "print(\"✅ Silver table created cleanly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5c8421-ae15-49b1-b260-54dc17d82e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Validación de la agregación. \n",
    "WITH validacion AS (\n",
    "  SELECT\n",
    "    clave,\n",
    "    totales_min,\n",
    "    totales_max,\n",
    "\n",
    "    /* sums with null-safe coalesce */\n",
    "    COALESCE(imss_min,0) + COALESCE(issste_min,0) + COALESCE(pemex_min,0)\n",
    "    + COALESCE(imss_bienestar_min,0) + COALESCE(ccinshae_min,0) + COALESCE(salud_spps_min,0)\n",
    "      AS sum_min,\n",
    "\n",
    "    COALESCE(imss_max,0) + COALESCE(issste_max,0) + COALESCE(pemex_max,0)\n",
    "    + COALESCE(imss_bienestar_max,0) + COALESCE(ccinshae_max,0) + COALESCE(salud_spps_max,0)\n",
    "      AS sum_max\n",
    "  FROM workspace.default.silver_licitacion_info\n",
    "),\n",
    "\n",
    "deltas AS (\n",
    "  SELECT\n",
    "    clave,\n",
    "    totales_min,\n",
    "    sum_min,\n",
    "    (sum_min - COALESCE(totales_min,0)) AS delta_min,\n",
    "\n",
    "    totales_max,\n",
    "    sum_max,\n",
    "    (sum_max - COALESCE(totales_max,0)) AS delta_max\n",
    "  FROM validacion\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM deltas\n",
    "WHERE delta_min <> 0 OR delta_max <> 0\n",
    "ORDER BY ABS(delta_min) DESC, ABS(delta_max) DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7201df-f365-4493-856b-49b7de2fe1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_dir = \"dbfs:/Volumes/workspace/default/eseotres/silver_licitacion_info_export_csv\"\n",
    "\n",
    "(spark.table(\"workspace.default.silver_licitacion_info\")\n",
    " .coalesce(1)  # single CSV file (ok if not huge)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(out_dir))\n",
    "\n",
    "print(\"✅ Exported to:\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ef2c8a0-8888-44d8-a02f-b4a29798ff42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8966909512757447,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Consolidada 2027-2028",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
