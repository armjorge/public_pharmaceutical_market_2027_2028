{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf5b59c-584b-4e20-905c-e64e5c881de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AnÃ¡lisis de la demanda agregada para la compra 2027-2028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eceb6d4-0366-461d-aebc-7298dae7230e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3918b3d6-f7d0-470a-a240-c3a629d45cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MÃ³dulos\n",
    "%pip install openpyxl\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "import operator\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e6f1c6-ac64-4f98-80f2-32e8e97b1328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Read messy Excel as raw grid (string-first)\n",
    "# -----------------------------\n",
    "xlsx_path = \"/Volumes/workspace/default/eseotres/AnÃ¡lisis Plataforma Salud.xlsx\"\n",
    "\n",
    "pdf = pd.read_excel(\n",
    "    xlsx_path,\n",
    "    sheet_name=\"Info licitaciÃ³n\",\n",
    "    header=None,\n",
    "    dtype=str,\n",
    "    na_filter=False\n",
    ")\n",
    "\n",
    "# Add a true row index BEFORE Spark (preserves order 1:1 with Excel)\n",
    "pdf[\"_row_id\"] = range(len(pdf))\n",
    "\n",
    "# 2) Convert to Spark (keep your existing schema build, but include _row_id)\n",
    "data = pdf.astype(str).values.tolist()  # ok, but we must treat \"nan\" as empty later\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(f\"_c{i}\", StringType(), True) for i in range(pdf.shape[1] - 1)] +\n",
    "    [StructField(\"_row_id\", StringType(), True)]  # as string for now; we cast below\n",
    ")\n",
    "\n",
    "df_bronze = spark.createDataFrame(data, schema=schema).withColumn(\"_row_id\", F.col(\"_row_id\").cast(\"long\"))\n",
    "\n",
    "# 3) Drop ONLY rows that are entirely empty (treat \"\", null, \"nan\", \"none\" as empty)\n",
    "data_cols = [c for c in df_bronze.columns if c != \"_row_id\"]\n",
    "\n",
    "is_empty_col = lambda c: (\n",
    "    F.lower(F.trim(F.coalesce(F.col(c), F.lit(\"\")))).isin(\"\", \"nan\", \"none\", \"null\")\n",
    ")\n",
    "\n",
    "all_empty_expr = reduce(lambda a, b: a & b, [is_empty_col(c) for c in data_cols])\n",
    "\n",
    "df_bronze = df_bronze.filter(~all_empty_expr)\n",
    "\n",
    "# Now you can inspect in the real order:\n",
    "df_bronze.orderBy(\"_row_id\").limit(30).show(truncate=False)\n",
    "print(\"Spark bronze cols:\", len(df_bronze.columns), \"| rows:\", df_bronze.count())\n",
    "\n",
    "# 4) Persist Bronze Delta table\n",
    "# (optional but safest) drop the old table first\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.bronze_licitacion_info\")\n",
    "\n",
    "# write the new one\n",
    "(df_bronze.write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(\"workspace.default.bronze_licitacion_info\"))\n",
    "\n",
    "print(\"âœ… Created table: workspace.default.bronze_licitacion_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad33757-efd1-4160-9de6-053e68ff7ade",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_c1\":143,\"_c8\":90},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767051779382}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM \n",
    "  workspace.default.bronze_licitacion_info\n",
    "ORDER BY\n",
    "  _row_id\n",
    "LIMIT \n",
    "  10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f37667-1cdc-4f18-a44c-16097c5d5740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4569d7e1-460d-4716-ad85-65ced91ced65",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"descripcion\":189,\"imss_bienestar_min\":137,\"imss_bienestar_max\":134},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767051912940}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def norm(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = (s.replace(\"Ã¡\",\"a\").replace(\"Ã©\",\"e\").replace(\"Ã­\",\"i\")\n",
    "          .replace(\"Ã³\",\"o\").replace(\"Ãº\",\"u\").replace(\"Ã±\",\"n\"))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def to_double(colname):\n",
    "    return F.when(\n",
    "        F.trim(F.coalesce(F.col(colname), F.lit(\"\"))) == \"\", None\n",
    "    ).otherwise(\n",
    "        F.regexp_replace(F.col(colname), \",\", \"\").cast(\"double\")\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load Bronze\n",
    "# -----------------------------\n",
    "df_bronze = spark.table(\"workspace.default.bronze_licitacion_info\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Detect header rows\n",
    "# -----------------------------\n",
    "data_cols = [c for c in df_bronze.columns if c != \"_row_id\"]\n",
    "\n",
    "contains_clave = reduce(\n",
    "    lambda a, b: a | b,\n",
    "    [F.lower(F.col(c)).contains(\"clave\") for c in data_cols]\n",
    ")\n",
    "\n",
    "header_row_id = df_bronze.where(contains_clave).select(F.min(\"_row_id\")).first()[0]\n",
    "group_row_id  = header_row_id - 1\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Extract header rows to pandas\n",
    "# -----------------------------\n",
    "pdf_head = (\n",
    "    df_bronze\n",
    "    .where(F.col(\"_row_id\").isin([group_row_id, header_row_id]))\n",
    "    .orderBy(\"_row_id\")\n",
    "    .drop(\"_row_id\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "r_group = pdf_head.iloc[0].replace([\"\", \"nan\", \"None\", None], pd.NA).ffill().fillna(\"\")\n",
    "r_head  = pdf_head.iloc[1].fillna(\"\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build FINAL column names (allow duplicates)\n",
    "# -----------------------------\n",
    "final_cols = []\n",
    "for g, h in zip(r_group.tolist(), r_head.tolist()):\n",
    "    g0 = norm(g)\n",
    "    h0 = norm(h)\n",
    "\n",
    "    if h0 in (\"clave\", \"descripcion\"):\n",
    "        final_cols.append(h0)\n",
    "    elif h0 in (\"min\", \"max\") and g0:\n",
    "        final_cols.append(f\"{g0}_{h0}\")\n",
    "    else:\n",
    "        final_cols.append(\"col\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Create structured Silver base\n",
    "# -----------------------------\n",
    "df_data = (\n",
    "    df_bronze\n",
    "    .where(F.col(\"_row_id\") > header_row_id)\n",
    "    .drop(\"_row_id\")\n",
    "    .toDF(*final_cols)\n",
    ")\n",
    "# --- define aggregation prefixes ---\n",
    "# Define the aggregation families\n",
    "AGG_FAMILIES = [\"imss_bienestar\", \"ccinshae\", \"salud_spps\"]\n",
    "\n",
    "# Start with your existing dataframe\n",
    "df_aggregated = df_data\n",
    "\n",
    "# For each family, find matching columns and aggregate them\n",
    "for family in AGG_FAMILIES:\n",
    "    # Find columns that match the pattern: family_*_min and family_*_max\n",
    "    min_cols = [col for col in df_data.columns if col.startswith(f\"{family}_\") and col.endswith(\"_min\")]\n",
    "    max_cols = [col for col in df_data.columns if col.startswith(f\"{family}_\") and col.endswith(\"_max\")]\n",
    "    \n",
    "    # Sum these columns to create aggregated columns\n",
    "    if min_cols:\n",
    "        # Create sum expression for min columns\n",
    "        min_sum_expr = sum([F.col(c) for c in min_cols])\n",
    "        df_aggregated = df_aggregated.withColumn(f\"{family}_min\", min_sum_expr)\n",
    "    \n",
    "    if max_cols:\n",
    "        # Create sum expression for max columns\n",
    "        max_sum_expr = sum([F.col(c) for c in max_cols])\n",
    "        df_aggregated = df_aggregated.withColumn(f\"{family}_max\", max_sum_expr)\n",
    "\n",
    "# Now select only the columns you need\n",
    "# Get the base columns\n",
    "base_cols = ['clave', 'descripcion', 'imss_min', 'imss_max', 'issste_min', 'issste_max', \n",
    "             'pemex_min', 'pemex_max']\n",
    "\n",
    "# Agregar columnas de familias\n",
    "for family in AGG_FAMILIES:\n",
    "    base_cols.extend([f\"{family}_min\", f\"{family}_max\"])\n",
    "\n",
    "# Agregar totales\n",
    "base_cols.extend(['totales_min', 'totales_max'])\n",
    "\n",
    "# Limpiar descripcion: remover comillas y normalizar espacios\n",
    "df_clean = df_aggregated.withColumn(\n",
    "    'descripcion',\n",
    "    F.regexp_replace(F.col('descripcion'), '\"', '')  # Quitar comillas dobles\n",
    ").withColumn(\n",
    "    'descripcion',\n",
    "    F.regexp_replace(F.col('descripcion'), \"'\", '')  # Quitar comillas simples\n",
    ").withColumn(\n",
    "    'descripcion',\n",
    "    F.trim(F.col('descripcion'))  # Quitar espacios al inicio/final\n",
    ").withColumn(\n",
    "    'descripcion',\n",
    "    F.regexp_replace(F.col('descripcion'), '\\\\s+', ' ')  # Normalizar mÃºltiples espacios a uno solo\n",
    ")\n",
    "\n",
    "# Seleccionar SOLO las columnas que necesitamos\n",
    "df_silver_clean = df_clean.select(*base_cols)\n",
    "\n",
    "df_silver_clean.display()\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Persist Silver\n",
    "# -----------------------------\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.silver_licitacion_info\")\n",
    "\n",
    "(df_silver_clean.write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(\"workspace.default.silver_licitacion_info\"))\n",
    "\n",
    "print(\"âœ… Silver table created cleanly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5c8421-ae15-49b1-b260-54dc17d82e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ValidaciÃ³n de la agregaciÃ³n. \n",
    "WITH validacion AS (\n",
    "  SELECT\n",
    "    clave,\n",
    "    totales_min,\n",
    "    totales_max,\n",
    "\n",
    "    /* sums with null-safe coalesce */\n",
    "    COALESCE(imss_min,0) + COALESCE(issste_min,0) + COALESCE(pemex_min,0)\n",
    "    + COALESCE(imss_bienestar_min,0) + COALESCE(ccinshae_min,0) + COALESCE(salud_spps_min,0)\n",
    "      AS sum_min,\n",
    "\n",
    "    COALESCE(imss_max,0) + COALESCE(issste_max,0) + COALESCE(pemex_max,0)\n",
    "    + COALESCE(imss_bienestar_max,0) + COALESCE(ccinshae_max,0) + COALESCE(salud_spps_max,0)\n",
    "      AS sum_max\n",
    "  FROM workspace.default.silver_licitacion_info\n",
    "),\n",
    "\n",
    "deltas AS (\n",
    "  SELECT\n",
    "    clave,\n",
    "    totales_min,\n",
    "    sum_min,\n",
    "    (sum_min - COALESCE(totales_min,0)) AS delta_min,\n",
    "\n",
    "    totales_max,\n",
    "    sum_max,\n",
    "    (sum_max - COALESCE(totales_max,0)) AS delta_max\n",
    "  FROM validacion\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM deltas\n",
    "WHERE delta_min <> 0 OR delta_max <> 0\n",
    "ORDER BY ABS(delta_min) DESC, ABS(delta_max) DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7201df-f365-4493-856b-49b7de2fe1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_dir = \"dbfs:/Volumes/workspace/default/eseotres/silver_licitacion_info_export_csv\"\n",
    "\n",
    "(spark.table(\"workspace.default.silver_licitacion_info\")\n",
    " .coalesce(1)  # single CSV file (ok if not huge)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(out_dir))\n",
    "\n",
    "print(\"âœ… Exported to:\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ef2c8a0-8888-44d8-a02f-b4a29798ff42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Golden Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08254f23-a3ed-4726-8d62-dfa0a2f1d203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read silver table\n",
    "df_silver = spark.table(\"workspace.default.silver_licitacion_info\")\n",
    "\n",
    "# Step 1: Select only the columns we need\n",
    "max_columns = [c for c in df_silver.columns if c.endswith('_max') and not c.startswith('totales')]\n",
    "\n",
    "print(f\"ðŸ“Š Institution columns found: {max_columns}\")\n",
    "\n",
    "# Step 2: Fill nulls with 0 and ensure all columns are BIGINT\n",
    "df_selected = df_silver.select(['clave', 'descripcion'] + max_columns)\n",
    "\n",
    "for col_name in max_columns:\n",
    "    df_selected = df_selected.withColumn(\n",
    "        col_name,\n",
    "        F.coalesce(col(col_name).cast('bigint'), F.lit(0))\n",
    "    )\n",
    "\n",
    "# Step 3: Build the CORRECT stack expression\n",
    "# stack(N, 'name1', value1, 'name2', value2, ...) - alternating string/value pairs\n",
    "stack_expr = f\"stack({len(max_columns)}\"\n",
    "\n",
    "for col_name in max_columns:\n",
    "    institution_name = col_name.replace('_max', '')\n",
    "    # This is the key fix: wrap column name in backticks for SQL expression\n",
    "    stack_expr += f\", '{institution_name}', `{col_name}`\"\n",
    "\n",
    "stack_expr += \") as (institucion, cantidad)\"\n",
    "\n",
    "print(f\"\\nðŸ”§ Stack expression: {stack_expr[:200]}...\")  # Debug: see first 200 chars\n",
    "\n",
    "# Step 4: Apply the transformation\n",
    "df_golden_cross = df_selected.select(\n",
    "    'clave',\n",
    "    'descripcion',\n",
    "    expr(stack_expr)\n",
    ")\n",
    "\n",
    "# Step 5: Clean up - remove null/zero quantities (optional)\n",
    "df_golden_cross = df_golden_cross.filter(\n",
    "    (col('cantidad').isNotNull()) & \n",
    "    (col('cantidad') > 0)\n",
    ")\n",
    "\n",
    "# Step 6: Verify the result\n",
    "print(\"\\nâœ… Golden Cross Table Schema:\")\n",
    "df_golden_cross.printSchema()\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample data for one product:\")\n",
    "sample_clave = df_golden_cross.select('clave').first()[0]\n",
    "df_golden_cross.filter(col('clave') == sample_clave).show(truncate=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š Total rows: {df_golden_cross.count()}\")\n",
    "\n",
    "# Step 7: Save as golden table\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.gold_institution_distribution\")\n",
    "\n",
    "df_golden_cross.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"workspace.default.gold_institution_distribution\")\n",
    "\n",
    "print(\"âœ… Golden table created: gold_institution_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e279ce-afc7-40e1-9750-82dae8fa666d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Exportar tabla golden cruzada\n",
    "out_dir = \"dbfs:/Volumes/workspace/default/eseotres\"\n",
    "\n",
    "(spark.table(\"workspace.default.gold_institution_distribution\")\n",
    " .coalesce(1)  # single CSV file (ok if not huge)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(out_dir))\n",
    "\n",
    "print(\"âœ… Exported to:\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Consolidada 2027-2028",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
