{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeccd6ba-424e-468f-9712-5b3b0686b1bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importar Librerías"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a777307-10b0-4320-a7f9-b43e3e096c2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generar Headers"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path al archivo de interés 2025-2026.xlsx\n",
    "xlsx_path = os.path.abspath(os.path.join(\"..\", \"source_files\", \"2025-2026.xlsx\"))\n",
    "\n",
    "# Cargar como dataframe\n",
    "pdf = pd.read_excel(\n",
    "    xlsx_path,\n",
    "    header=None,\n",
    "    dtype=str,\n",
    "    na_filter=False\n",
    ")\n",
    "\n",
    "\n",
    "# Obtener el # de fila con las palabras ['2025', 'Cantidad Solicitada', 'Mínimo']\n",
    "place_holders = ['2025', 'Cantidad Solicitada', 'Mínimo']\n",
    "# Obtener headers para la tabla Bronze\n",
    "# 1. Obtenemos el índice basado para cada place holder\n",
    "_p1 = pdf.index[pdf.iloc[:, 8] == place_holders[0]]\n",
    "_p2 = pdf.index[pdf.iloc[:, 8] == place_holders[1]]\n",
    "_p3 = pdf.index[pdf.iloc[:, 8] == place_holders[2]]\n",
    "\n",
    "# 2. Extramos filas, reemplazando \"\" por nan\n",
    "header_p1 = pdf.iloc[_p1].replace(\"\", pd.NA)\n",
    "header_p2 = pdf.iloc[_p2].replace(\"\", pd.NA)\n",
    "header_p3 = pdf.iloc[_p3].replace(\"\", pd.NA)\n",
    "\n",
    "\n",
    "# 3. Llenamos filas faltantes en el rango de columnas 8-11 y 16 a 20 de la fila 2\n",
    "header_p2.iloc[:, 5:12] = header_p2.iloc[:, 5:12].ffill(axis=1)\n",
    "header_p2.iloc[:, 16:] = header_p2.iloc[:, 16:].ffill(axis=1)\n",
    "\n",
    "# 4. Lleamos las Llenamos filas faltantes en el rango de columnas 8-12 y 16 al final de la fila 1\n",
    "header_p1.iloc[:, 8:12] = header_p1.iloc[:, 8:12].ffill(axis=1)\n",
    "header_p1.iloc[:, 16:] = header_p1.iloc[:, 16:].ffill(axis=1)\n",
    "\n",
    "# 5. Combinamos los dataframes y transponemos, para después combinarlos, la columna header_p1 con los años va al final.\n",
    "df_headers = pd.concat([header_p3.T, header_p2.T, header_p1.T], axis=1)\n",
    "headers_raw = df_headers.apply(\n",
    "    lambda row: ' '.join(row.dropna().astype(str)) if row.notna().any() else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 6. Limpiamos \n",
    "\n",
    "vocales = str.maketrans({'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u'})\n",
    "\n",
    "headers = (\n",
    "    headers_raw  \n",
    "    .astype(str)                            # ensure strings\n",
    "    .str.strip()                            # trim whitespace\n",
    "    .str.lower()                            # lowercase\n",
    "    .str.replace(\"cantidad solicitada\", \"\", regex=True)\n",
    "    .str.replace(r\"[\\s+.]+\", \"_\", regex=True)\n",
    "    .str.translate(vocales)\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Convertimos a lista\n",
    "headers_clean = headers.tolist()\n",
    "print(f\"✅ Headers extraídos exitosamente: {headers_clean[0:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f09fc54c-e5e4-41ee-a453-d37b0780fed2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Crear tabla Bronze e Ingestión"
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazamos valores vacíos y no relevantes de la primer columna por nan\n",
    "\n",
    "pdf.iloc[:, 0] = pdf.iloc[:, 0].replace({'No. Partida': np.nan, '': np.nan, '.': np.nan})\n",
    "\n",
    "# Retiramos los nan basados en la primera columna\n",
    "\n",
    "data_pdf = pdf.dropna(subset=[pdf.columns[0]])\n",
    "\n",
    "# Creamos el dataframe de spark\n",
    "spark_df = spark.createDataFrame(data_pdf) \\\n",
    "                 .toDF(*headers_clean)\n",
    "\n",
    "# Guardamos la tabla Delta\n",
    "table = \"workspace.default.bronze_2025_2026\"\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(table)\n",
    "\n",
    "print(f\"✅ Tabla e ingestión completada: {table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_Bronze_2025_2026",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
